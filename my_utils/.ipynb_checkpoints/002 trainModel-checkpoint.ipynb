{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28671226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch library to build neural network\n",
    "import torch  # Elementory function of tensor is define in torch package\n",
    "import torch.nn as nn # Several layer architectur is define here\n",
    "import torch.nn.functional as F # loss function and activation function\n",
    "\n",
    "\"\"\"\n",
    "Computer vision is one of the most important application and thus lots \n",
    "of deplopment in the and torch.vision provides many facilities that can \n",
    "be use to imporve model such as data augmentation, reading data batchwise, \n",
    "suffling data before each epoch and many more\n",
    "\"\"\"\n",
    "# import torch library related to image data processing\n",
    "import torchvision # provides facilities to access image dataset\n",
    "from torchvision.datasets.utils import download_url \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"\"\"Check if Cuda GPU is available\"\"\"\n",
    "#check for CUDA enabled GPU card\n",
    "def getDeviceType():\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')\n",
    "device = getDeviceType()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a02a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Training Process\n",
    "Below code will work as a base function and provide all the important \n",
    "function like compute loss, accuracy and print result in a perticular \n",
    "formate afte each epoch. Funvtion are as follow\n",
    "1. Accuracy : Computer accuracy in evalutaion mode of pytorch on given dataset for given model\n",
    "2. compute_batch_loss : Compute batch loss and append the loss in the list of batch loss.\n",
    "3. compute_batch_loss_acc : Compute batch loss, batch accuracy and append the loss in the list of batch loss.\n",
    "4. accumulate_batch_loss_acc: Accumulate loss from the list of batch and acccuraly loss.\n",
    "5. Epoch end to print the output after every epoch in proper format\n",
    "\"\"\"\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) \t\t# get the prediction vector\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Compute loss of the given batch and return it\n",
    "def compute_batch_loss(newmodel, batch_X,batch_y):\n",
    "  images = batch_X.to(device)\n",
    "  labels = batch_y.to(device)\n",
    "  out = newmodel(images)                  \t\t# Generate predictions\n",
    "  loss = F.cross_entropy(out, labels) \t\t\t# Calculate loss\n",
    "  return loss\n",
    "\n",
    "# Computes loss and accuracy of the given batch(Used in validation)\n",
    "def compute_batch_loss_acc(newmodel, batch_X,batch_y):\n",
    "    images = batch_X.to(device)\n",
    "    labels = batch_y.to(device)\n",
    "    out = newmodel(images)                    \t# Generate predictionsin_features=4096\n",
    "    loss = F.cross_entropy(out, labels)   \t\t# Calculate loss\n",
    "    acc = accuracy(out, labels)           \t\t# Calculate accuracy\n",
    "    return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "# At the end of epoch accumulate all batch loss and batch accueacy    \n",
    "def accumulate_batch_loss_acc(outputs):\n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(epoch, result):\n",
    "  # Print in given format \n",
    "  # Epoch [0], last_lr: 0.00278, train_loss: 1.2862, val_loss: 1.2110, val_acc: 0.6135\n",
    "  strResult = \"Epoch [{}], last_lr: {:.6f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "      epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "  #print(strResult)\n",
    "  return strResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3919f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Define Training \n",
    "Here we will evalute our model after each epoch on validation dataset using evalute method\n",
    "get_lr method returnd last learning rate used in the training\n",
    "Here we are using one fit cycle method in which we specify the max learning rate and learning \n",
    "rate start from 1/10th value of max_lr and slowly increases the value to max_lr for 40% of updates \n",
    "then decreases to its initial value for 40% updates and then further decreases to 1/100th of max_lr \n",
    "value to perform final fine tuning.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# evalute model on given dataset using given data loader\n",
    "@torch.no_grad()\n",
    "# evalute model on given dataset using given data loader\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch_X, batch_y in data_loader:\n",
    "        outputs = [compute_batch_loss_acc(model,batch_X,batch_y)]\n",
    "      return accumulate_batch_loss_acc(outputs)\n",
    "\n",
    "# Use special scheduler to change the value of learning rate\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# epoch=8, max_lr=.01, weight_decay(L2-Regu parametr)=.0001,opt_func=Adam\n",
    "\n",
    "######### Main Function To Implement Training #################\n",
    "def fit_one_cycle(dataloaders,trainDir,testDir,\n",
    "                  ModelName, model, device_l=torch.device('cpu'), \n",
    "                  epochs, max_lr, weight_decay=0, L1=0,grad_clip=None, opt_func=torch.optim.SGD,logFile=''):\n",
    "    torch.cuda.empty_cache()\n",
    "    global device\n",
    "    device = device_l\n",
    "    history = []\n",
    "    # Set up cutom optimizer here we will use one cycle scheduler with max learning\n",
    "    # rate given by max_lr, default optimizer is SGD but we will use ADAM, and \n",
    "    # L2 Regularization using weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(dataloaders[trainDir]))\n",
    "    print(\"Training Starts\")\n",
    "    with open(logFile, \"a\") as f:\n",
    "      for epoch in range(epochs):\n",
    "          # Training Phase \n",
    "          model.train()  #######################\n",
    "          train_losses = []\n",
    "          lrs = []\n",
    "          #for batch in train_loader:\n",
    "          for batch_X, batch_y in dataloaders[trainDir]:\n",
    "              # computer the training loss of current batch\n",
    "              loss = compute_batch_loss(model,batch_X,batch_y)\n",
    "              l1_crit = nn.L1Loss()\n",
    "              reg_loss = 0\n",
    "              for param in model.parameters():\n",
    "                reg_loss += l1_crit(param,target=torch.zeros_like(param))\n",
    "              loss += L1*reg_loss \n",
    "              \n",
    "              train_losses.append(loss)\n",
    "              loss.backward() # compute the gradient of all weights\n",
    "              # Clip the gradient value to maximum allowed grad_clip value\n",
    "              if grad_clip: \n",
    "                  nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "              optimizer.step() # Updates weights \n",
    "              # pytorch by default accumulate grade history and if we dont want it\n",
    "              # we should make all previous grade value equals to zero\n",
    "              optimizer.zero_grad() \n",
    "              # Record & update learning rate\n",
    "              lrs.append(get_lr(optimizer))\n",
    "              sched.step() # Update the learning rate\n",
    "              # Compute Validation Loss and Valodation Accuracy\n",
    "              result = evaluate(model, dataloaders[testDir])\n",
    "              # Compute Train Loss of whole epoch i.e mean of loss of batch \n",
    "              result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "              # Observe how learning rate is change by schedular\n",
    "              result['lrs'] = lrs\n",
    "              # print the observation of each epoch in a proper format\n",
    "          \n",
    "          #strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, \n",
    "            #val_acc: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "          strResult = epoch_end(epoch, result) \n",
    "          \n",
    "          f.write(f\"{ModelName}-\\t{strResult}\\n\")\n",
    "          print(strResult)\n",
    "          history.append(result) # append tupple result with val_acc, vall_loss, and trin_loss\n",
    "        \n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
